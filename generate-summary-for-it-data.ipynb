{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7724489,"sourceType":"datasetVersion","datasetId":4512622}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## install dependancies","metadata":{}},{"cell_type":"code","source":"# install Hugging Face Libraries\n!pip install peft\n!pip install transformers datasets accelerate evaluate bitsandbytes loralib --upgrade --quiet\n# install additional dependencies needed for training\n!pip install rouge-score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os \nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading data","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"ANWAR101/youtube-cnn\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.remove_columns('Unnamed: 0')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.shuffle(seed = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert text to text to token IDs","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_id=\"facebook/bart-base\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_input_length = 1024\nmax_target_length = 600\n\n\ndef preprocess_function(examples):\n    model_inputs = tokenizer(\n        examples[\"text\"],\n        max_length=max_input_length,\n        truncation=True,\n    )\n    labels = tokenizer(\n        examples[\"summary\"], max_length=max_target_length, truncation=True\n    )\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=ds['train'].column_names)\nprint(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-Tune with LoRA","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM\nimport torch\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n\n# Define LoRA Config\nlora_config = LoraConfig(\n r = 8,\n lora_alpha = 16,\n target_modules=[\"q_proj\", \"v_proj\"],\n lora_dropout=0.05,\n bias=\"none\",\n task_type=TaskType.SEQ_2_SEQ_LM\n)\n\n\n# prepare int-8 model for training\nmodel = prepare_model_for_int8_training(model)\n\n# add LoRA adaptor\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer,model=model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token = \"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\noutput_dir=\"lora-bart-base-fine-tuned-youtube-cnn-3\"\n\nbatch_size = 8\nnum_train_epochs = 8\n# Show the training loss with every epoch\nlogging_steps = len(tokenized_dataset[\"train\"]) // batch_size\nmodel_name = model_id.split(\"/\")[-1]\n\nargs = Seq2SeqTrainingArguments(\n    output_dir=output_dir,\n    evaluation_strategy=\"epoch\",\n    learning_rate=1e-3,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=num_train_epochs,\n    predict_with_generate=True,\n    logging_steps=logging_steps,\n    gradient_accumulation_steps = 10,\n     warmup_steps=1000,  # Gradually increase learning rate for the first 1000 steps\n    lr_scheduler_type=\"linear\",  # Linearly decrease after warmup\n    push_to_hub=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install nltk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download(\"punkt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import evaluate\nrouge_score = evaluate.load(\"rouge\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    # Decode generated summaries into text\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    # Decode reference summaries into text\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    # ROUGE expects a newline after each sentence\n    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n    # Compute ROUGE scores\n    result = rouge_score.compute(\n        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n    )\n    # Extract the scores\n    result = {key: value * 100 for key, value in result.items()}\n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}